{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edde484",
   "metadata": {},
   "source": [
    "## Autoencoder V1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b72762",
   "metadata": {},
   "source": [
    "The following autoencoder is based on the following websites:\n",
    "\n",
    "https://medium.datadriveninvestor.com/covid-19-detection-in-x-ray-images-with-pytorch-5c5602b4658f\n",
    "https://medium.com/pytorch/implementing-an-autoencoder-in-pytorch-19baa22647d1\n",
    "https://gist.github.com/AFAgarap/4f8a8d8edf352271fa06d85ba0361f26#file-autoencoder-pytorch-ipynb\n",
    "\n",
    "https://discuss.pytorch.org/t/how-to-load-images-without-using-imagefolder/59999/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6acaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/SharedUsers/sdo19/home/chestx-rays\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fee93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/files\n"
     ]
    }
   ],
   "source": [
    "cd /media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d132765c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1f1ff02eb957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                     \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "import os\n",
    "from PIL import Image as PILIM\n",
    "#import cv2 #OPENCV, might be a third way to access images?\n",
    "#FOUND OUT NOT ALL IMAGES ARE SAME SIZE, USUALLY AROUND (2544,3056)\n",
    "#Found out there are 377110 images\n",
    "#Below I have 2 diff ways to display images, not sure which one i want, depends on what methods I wanna use? Where am I storing images?\n",
    "\n",
    "\n",
    "directory=\"/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/files\"\n",
    "count=0\n",
    "numimages=0\n",
    "im_names=[]\n",
    "name=\"\"\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)  \n",
    "    if os.path.isdir(f):\n",
    "        for foldername in os.listdir(f):\n",
    "            g = os.path.join(f, foldername) \n",
    "            if os.path.isdir(g):\n",
    "                for h in os.listdir(g):\n",
    "                    k=os.path.join(g,h)\n",
    "                    if os.path.isdir(k):\n",
    "                        for image1 in os.listdir(k):\n",
    "                            impath = os.path.join(k, image1) \n",
    "                            if impath.endswith(\"jpg\"):\n",
    "                                if count<10000:\n",
    "                                    #2 ways to get image\n",
    "                                    \n",
    "                                    #1ST WAY\n",
    "                                    \n",
    "                                    #currim=Image(filename=impath,width = 100, height = 100)\n",
    "                                    #display(currim)\n",
    "                                    name=impath\n",
    "                                    if directory in name:\n",
    "                                        name.replace(directory,'')\n",
    "                                    im_names.append(name)\n",
    "                                    \n",
    "                                    #2ND WAY\n",
    "                                    \n",
    "                                    #im3 = PILIM.open(impath)\n",
    "                                    \n",
    "                                    #Calculate size, only know how to do it with 2nd way\n",
    "                                    #size=im3.size\n",
    "                                    #print(size)\n",
    "                                    #display(im3)\n",
    "                                    #listimages.append(im3)\n",
    "                                    \n",
    "                                    count+=1\n",
    "                                numimages+=1\n",
    "print(im_names[0:10])\n",
    "print(numimages) #377110 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52afb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from PIL import Image\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb957745",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_name, test_img_name= train_test_split(im_names, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromImages(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_name, transforms=None): \n",
    "        self.image_arr = np.asarray(img_name)\n",
    "        #self.label_arr = np.asarray(label)\n",
    "        self.data_len = len(img_name)\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, index):\n",
    "        single_img_name = self.image_arr[index]\n",
    "        img_array = PILIM.open(single_img_name).convert('RGB') #THIS IS ASKING TO OPEN BEFORE I TRANSFORM?? is that it?  nope\n",
    "        if self.transforms is not None:   #I HAD Image.open, but i import image as PILIM(see above)\n",
    "            img_array = self.transforms(img_array)\n",
    "        #image_label = self.label_arr[index]\n",
    "        return (single_img_name, img_array)\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class CustomDatasetFromImages(torch.utils.data.Dataset):\n",
    "    #def __init__(self, img_name, transforms=None): \n",
    "       # self.image_arr = np.asarray(img_name)\n",
    "        #self.label_arr = np.asarray(label)\n",
    "        #self.data_len = len(img_name)\n",
    "        #self.transforms = transforms\n",
    "   # def __getitem__(self, index):\n",
    "        #single_img_name = self.image_arr[index]\n",
    "        #img_array = Image.open(single_img_name).convert('RGB')\n",
    "       # if self.transforms is not None:\n",
    "            #img_array = self.transforms(img_array)\n",
    "        #image_label = self.label_arr[index]\n",
    "        #tensor_image = self.transform(image)\n",
    "        #return tensor_image\n",
    "   # def __len__(self):\n",
    "        #return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c493c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Aug = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)),\n",
    "                                            torchvision.transforms.RandomRotation((-20, 20)),\n",
    "                                            #torchvision.transforms.RandomAffine(0, translate=None, scale=[0.7, 1.3], shear=None, resample=False, fillcolor=0),\n",
    "                                            torchvision.transforms.ToTensor()])\n",
    "test_Aug = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)),torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=CustomDatasetFromImages(train_img_name, transforms=train_Aug)\n",
    "test_set=CustomDatasetFromImages(test_img_name, transforms=test_Aug)\n",
    "trainloader= torch.utils.data.DataLoader(train_set, batch_size=64, num_workers=2,shuffle=True) \n",
    "testloader= torch.utils.data.DataLoader(test_set, batch_size=64, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=128\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_hidden_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.sigmoid(code)\n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.sigmoid(activation)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_shape=50176).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f42297",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for _,batch_features in trainloader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.view(-1, 50176).to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(trainloader)\n",
    "    \n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for _,batch_features in testloader:\n",
    "        batch_features = batch_features[0:50]\n",
    "        test_examples = batch_features.view(-1, 50176)\n",
    "        reconstruction = model(test_examples)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    number = 50\n",
    "    #plt.figure(figsize=(20, 4))\n",
    "    for index in range(number):\n",
    "        # display original\n",
    "        #ax = plt.subplot(2, number, index + 1)\n",
    "        plt.figure()\n",
    "        plt.imshow(test_examples[index].numpy().reshape(224, 224))\n",
    "        plt.gray()\n",
    "        #ax.get_xaxis().set_visible(False)\n",
    "        #ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        #ax = plt.subplot(2, number, index + 1 + number)\n",
    "        plt.figure()\n",
    "        plt.imshow(reconstruction[index].numpy().reshape(224, 224))\n",
    "        plt.gray()\n",
    "        #ax.get_xaxis().set_visible(False)\n",
    "        #ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317890d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
