{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0bd107",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95108dc4",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "##### VAE Specific\n",
    "\n",
    "https://debuggercafe.com/convolutional-variational-autoencoder-in-pytorch-on-mnist-dataset/\n",
    "\n",
    "https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/\n",
    "\n",
    "\n",
    "##### CNN in General\n",
    "\n",
    "Testing/Validation: https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/cifar-cnn/cifar10_cnn_solution.ipynb\n",
    "\n",
    "Theory: https://cs231n.github.io/convolutional-networks/#conv\n",
    "\n",
    "Theory/AE Class: https://github.com/udacity/deep-learning-v2-pytorch/blob/master/autoencoder/convolutional-autoencoder/Convolutional_Autoencoder_Solution.ipynb\n",
    "\n",
    "Theory/AE Class: https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/cifar-cnn/cifar10_cnn_solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c339f9",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c483bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "#import pydicom\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import essential libraries\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from PIL import Image\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5177a",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4b4ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962</td>\n",
       "      <td>files/p10/p10000032/s50414267/174413ec-4ec4c1f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c</td>\n",
       "      <td>files/p10/p10000032/s53189527/e084de3b-be89b11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  study_id                                      dicom_id  \\\n",
       "0    10000032  50414267  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014   \n",
       "1    10000032  50414267  174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962   \n",
       "2    10000032  53189527  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab   \n",
       "3    10000032  53189527  e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c   \n",
       "4    10000032  53911762  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714   \n",
       "\n",
       "                                                path  \n",
       "0  files/p10/p10000032/s50414267/02aa804e-bde0afd...  \n",
       "1  files/p10/p10000032/s50414267/174413ec-4ec4c1f...  \n",
       "2  files/p10/p10000032/s53189527/2a2277a9-b0ded15...  \n",
       "3  files/p10/p10000032/s53189527/e084de3b-be89b11...  \n",
       "4  files/p10/p10000032/s53911762/68b5c4b1-227d048...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cxr_root_path = \"/media/Data/ComputerVision/mimic_cxr_jpg_small_chest_radiograph/data/physionet.org/files/mimic-cxr-jpg/2.0.0/\"\n",
    "record_df = pd.read_csv('{}/cxr-record-list.csv.gz'.format(cxr_root_path), sep=',')\n",
    "record_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd8066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377110\n"
     ]
    }
   ],
   "source": [
    "df_split = pd.read_csv('{}/mimic-cxr-2.0.0-split.csv.gz'.format(cxr_root_path))\n",
    "dataset = df_split.merge(record_df, on=['subject_id', 'study_id', 'dicom_id'], how='left')\n",
    "num_images=len(dataset)\n",
    "print(num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b275b7",
   "metadata": {},
   "source": [
    "### Selecting Portion of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c48143db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a2c2e",
   "metadata": {},
   "source": [
    "### Selecting Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de0d798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee08497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromImages(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, transforms=None): \n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, index):\n",
    "        imagepath=self.dataframe[\"path\"].iloc[index]\n",
    "        imagepath=cxr_root_path+imagepath[:-4]+'.jpg'\n",
    "        image=Image.open(imagepath)\n",
    "        if self.transforms is not None:\n",
    "            image=self.transforms(image)\n",
    "        return image\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c37a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Aug = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\n",
    "                                            torchvision.transforms.RandomRotation((-20, 20)),\n",
    "                                            torchvision.transforms.ToTensor()])\n",
    "test_Aug = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64280ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=CustomDatasetFromImages(dataset.loc[dataset['split']=='train'], transforms=train_Aug)\n",
    "test_set=CustomDatasetFromImages(dataset.loc[dataset['split']=='validate'], transforms=test_Aug)\n",
    "trainloader= torch.utils.data.DataLoader(train_set, batch_size=64, num_workers=6,shuffle=True) \n",
    "testloader= torch.utils.data.DataLoader(test_set, batch_size=64, num_workers=6,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb632cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#variables\n",
    "kernel_size = 4 # (4, 4) kernel\n",
    "init_channels = 8 # initial number of filters\n",
    "image_channels = 1 # images are grayscale\n",
    "latent_dim = 16 # latent dimension for sampling\n",
    "\n",
    "# define a Conv VAE\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=image_channels, out_channels=init_channels, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_channels, out_channels=init_channels*2, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_channels*2, out_channels=init_channels*4, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_channels*4, out_channels=64, kernel_size=kernel_size, \n",
    "            stride=2, padding=0\n",
    "        )\n",
    "        # fully connected layers for learning representations\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(128, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, 64)\n",
    "        # decoder \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=64, out_channels=init_channels*8, kernel_size=kernel_size, \n",
    "            stride=1, padding=0\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*8, out_channels=init_channels*4, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*4, out_channels=init_channels*2, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*2, out_channels=image_channels, kernel_size=kernel_size, \n",
    "            stride=2, padding=1\n",
    "        )\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        batch, _, _, _ = x.shape #no idea\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1) #no idea what this does \n",
    "        hidden = self.fc1(x)\n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.fc_mu(hidden)\n",
    "        log_var = self.fc_log_var(hidden)\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var) #sample\n",
    "        z = self.fc2(z) #no idea why \n",
    "        z = z.view(-1, 64, 1, 1)\n",
    " \n",
    "        # decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        reconstruction = torch.sigmoid(self.dec4(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837b746",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da40a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_loss=BCE + KLD\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5923585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "647bdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model=ConvVAE().to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b5c15",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d574732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 44262.680767 \tValidation Loss: 11003.366211\n",
      "Validation loss decreased (inf --> 11003.366211).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 42097.716638 \tValidation Loss: 10384.607422\n",
      "Validation loss decreased (11003.366211 --> 10384.607422).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 40532.726594 \tValidation Loss: 10132.836914\n",
      "Validation loss decreased (10384.607422 --> 10132.836914).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 39517.150732 \tValidation Loss: 10158.329102\n",
      "Epoch: 5 \tTraining Loss: 38985.376151 \tValidation Loss: 10069.240234\n",
      "Validation loss decreased (10132.836914 --> 10069.240234).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "train_loss_list=[]\n",
    "valid_loss_list=[]\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        # clear the gradients of all optimized variables\n",
    "        data=data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        \n",
    "    # validate the model #\n",
    "    model.eval()\n",
    "    for data in testloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        data=data.to(device)\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(testloader.sampler)\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        #torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87ddd4",
   "metadata": {},
   "source": [
    "### Plotting Loss Function across epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "600382f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(train_loss_list[0])\n",
    "plt.plot(range(0,5),train_loss_list,'bo')\n",
    "plt.plot(range(0,5),valid_loss_list,'ro')\n",
    "plt.legend(['Training Data','Validation Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9185bb0",
   "metadata": {},
   "source": [
    "### Plotting validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a78365",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1024 into shape (224,224)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/media/SharedUsers/sdo19/home/anaconda3/envs/torch171/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reshape() got an unexpected keyword argument 'order'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b3dd402f1e78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/media/SharedUsers/sdo19/home/anaconda3/envs/torch171/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    297\u001b[0m            [5, 6]])\n\u001b[1;32m    298\u001b[0m     \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/SharedUsers/sdo19/home/anaconda3/envs/torch171/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Call _wrapit from within the except clause to ensure a potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# exception has a traceback chain.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/SharedUsers/sdo19/home/anaconda3/envs/torch171/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1024 into shape (224,224)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # obtain one batch of test images\n",
    "    dataiter = iter(testloader)\n",
    "    images= dataiter.next()\n",
    "    images.numpy()\n",
    "\n",
    "    # move model inputs to cuda, if GPU available\n",
    "    images = images.cuda()\n",
    "\n",
    "    # get sample outputs\n",
    "    output = model(images)\n",
    "\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(10, 20))\n",
    "    for idx in range(5):\n",
    "        ax = fig.add_subplot(5,2, 1+2*idx)\n",
    "        im=np.reshape(images[idx].cpu(),(32,32))\n",
    "        ax.imshow(im,cmap='gray')\n",
    "        ax2 = fig.add_subplot(5,2, 2+2*idx)\n",
    "        im2=np.reshape(output[idx].cpu(),(32,32))\n",
    "        ax2.imshow(im2,cmap='gray')\n",
    "    fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed6167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
